---
title: "Bootstrapping standardized rates and decomposition effects"
format:
  html:
    embed-resources: true
---


The basic premise of bootstrapping Das Gupta's rate decomposition can be thought of as expanding out to unit-level data, re-sampling, then aggregating back up. 

Data from [Wang 2000](https://doi.org/10.3758/BF03207806):  
```{r}
#| message: false
#| warning: false
library(tidyverse)
library(DasGuptR)

eg.wang2000 <- data.frame(
  pop = rep(c("male", "female"), e = 12),
  ethnicity = rep(1:3, e = 4),
  age_group = rep(1:4, 3),
  size = c(
    130, 1305, 1539, 316, 211, 697, 334, 48, 105, 475, 424, 49,
    70, 604, 428, 43, 55, 127, 44, 9, 72, 178, 103, 12
  ),
  rate = c(
    12.31, 34.90, 52.91, 44.44, 16.67, 36.40, 51.20, 41.67, 12.38, 19.20, 21.23, 12.50,
    17.14, 35.55, 48.71, 55.81, 14.55, 39.37, 32.56, 55.56, 22.22, 20.34, 13.59, 8.33
  )
)
```

To do this with the DasGuptR package, we must first create a function to essentially "uncount" data into individual level data (where nrow = size of total population), then re-sample with replacement, then aggregate back up to the level of the original data.  

```{r}
getBS <- function() {
  # expand out
  exp <- eg.wang2000 |>
    mutate(
      r1 = (rate / 100) * size,
      r0 = (1 - (rate / 100)) * size
    ) |>
    select(pop, ethnicity, age_group, r0:r1) |>
    pivot_longer(r0:r1, names_to = "r", values_to = "n") |>
    mutate(n = as.integer(n)) |>
    uncount(n)

  # sample for each pop
  bs <- lapply(unique(eg.wang2000$pop), \(p)
  slice_sample(exp[exp$pop == p, ],
    prop = 1, replace = TRUE
  ) |>
    group_by(ethnicity, age_group) |>
    reframe(
      pop = p,
      size = n(),
      rate = mean(r == "r1") * 100
    ) |>
    ungroup())

  do.call(rbind, bs)
}
```

Take 500 resamples:  

```{r}
bsamps <-
  tibble(
    k = 1:500,
    i = map(1:500, ~ getBS())
  )
```

and apply the standardisation to each resample:  

```{r}
#| message: false
#| warning: false
bsamps <- bsamps |>
  mutate(
    dgo = map(i, ~ dgnpop(.,
      pop = "pop", factors = "rate",
      id_vars = c("ethnicity", "age_group"),
      crossclassified = "size"
    ))
  )
```

Here are the original difference effects:  
```{r}
#| message: false
dgnpop(eg.wang2000,
  pop = "pop", factors = "rate",
  id_vars = c("ethnicity", "age_group"),
  crossclassified = "size"
) |>
  dg_table()
```

And here are the standard errors:  

```{r}
bsamps |>
  select(k, dgo) |>
  unnest(dgo) |>
  group_by(k, factor) |>
  reframe(diff = rate[pop == "female"] - rate[pop == "male"]) |>
  group_by(factor) |>
  reframe(se = sd(diff))
```

Which we can take as a proportion of the crude rate difference:  

```{r}
#| echo: false
#| message: false
dd <- dgnpop(eg.wang2000,
  pop = "pop", factors = "rate",
  id_vars = c("ethnicity", "age_group"),
  crossclassified = "size"
) |>
  dg_table() |>
  rownames_to_column(var = "factor")

library(ggdist)
theme_set(theme_ggdist())
bsamps |>
  select(k, dgo) |>
  unnest(dgo) |>
  group_by(k, factor) |>
  reframe(decomp = (rate[pop == "male"] - rate[pop == "female"]) / dd$diff[4] * 100) |>
  filter(factor != "crude") |>
  ggplot(aes(x = decomp, y = factor, fill = factor)) +
  stat_halfeye() +
  scale_fill_viridis_d(option = "C", begin = 0.5, end = .9) +
  guides(fill = "none")
```


### Uncertainty in rates

We can use this same approach to estimate uncertainty in adjusted rates. For instance, using the Scottish Reconvictions data:  

```{r}
#| eval: false
data(reconv)

src <- reconv |>
  transmute(
    pop = year, sex = Sex, age = Age,
    rate = prev_rate,
    size = offenders,
    r1 = rate * size,
    r0 = (1 - rate) * size
  )

getBS <- function() {
  # expand out
  exp <- src |>
    select(pop, sex, age, r0, r1) |>
    pivot_longer(r0:r1, names_to = "r", values_to = "n") |>
    mutate(n = as.integer(n)) |>
    uncount(n)

  # sample for each pop
  bs <- lapply(unique(src$pop), \(p)
  slice_sample(exp[exp$pop == p, ],
    prop = 1, replace = TRUE
  ) |>
    group_by(sex, age) |>
    reframe(
      pop = p,
      size = n(),
      rate = mean(r == "r1")
    ) |>
    ungroup())

  do.call(rbind, bs)
}


# 500 resamples
bsamps <-
  tibble(
    k = 1:500,
    i = map(1:500, ~ getBS())
  )

# apply DG on each resample
bsamps <- bsamps |>
  mutate(
    dgo = map(i, ~ dgnpop(.,
      pop = "pop", factors = "rate",
      id_vars = c("age", "sex"),
      crossclassified = "size"
    ))
  )
```
```{r}
#| echo: false
bsamps <- readRDS("bsamps500.RDS")
bsamps |>
  unnest(dgo) |>
  mutate(pop = as.numeric(as.character(pop))) |>
  # group_by(pop,factor) |>
  # median_qi(rate, .width = c(.5,.8,.95)) |>
  # ggplot(aes(x = pop, y = rate, ymin = .lower, ymax = .upper)) +
  # geom_lineribbon(aes(group=factor)) +
  ggplot(aes(x = pop, y = rate, group = factor, fill = after_stat(.width))) +
  stat_lineribbon(.width = ppoints(50)) +
  facet_wrap(~factor) +
  scale_fill_distiller() +
  guides(fill = "none")
```


